# -*- coding: utf-8 -*-
"""UAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18bAXS9Dh4AEcWMXvew2PR489E-F7_cfM
"""

from google.colab import drive
drive.mount('/content/drive')

"""1. pengumpulan data"""

import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Tentukan jalur lengkap ke file di Google Drive
dir = '/content/drive/My Drive/hungarian.data'  # Sesuaikan jalur ini jika berbeda

# Baca file yang diunggah
with open(dir, encoding='Latin1') as file:
    lines = [line.strip() for line in file]

lines[0:10]

"""2. menelaah data"""

data = itertools.takewhile(
    lambda x: len(x) == 76,
    (' '.join(lines[i:(i+10)]).split() for i in range(0, len(lines), 10))
)

# Buat DataFrame dari data yang dipisahkan
df = pd.DataFrame.from_records(data)

df.head()

"""3. validasi data"""

# Ganti nilai -9.0 dengan NaN
df.replace('-9.0', np.nan, inplace=True)

# Tampilkan informasi umum untuk memeriksa penggantian
df.info()

# Tampilkan 5 baris pertama untuk memeriksa penggantian
df.head()

"""4. menentukan objek data"""

# Pilih kolom-kolom yang relevan
columns_to_use = [2, 3, 8, 9, 11, 15, 18, 31, 37, 39, 40, 43, 50, 57]

# Buat DataFrame baru dengan kolom yang dipilih
selected_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']
df_selected = df.iloc[:, columns_to_use]
df_selected.columns = selected_columns

# Tampilkan 5 baris pertama dari DataFrame baru
df_selected.head()

"""5.membersihkan data"""

# Ganti nilai -9.0 dengan NaN
df_selected.replace(-9.0, np.nan, inplace=True)

# Hapus baris dengan nilai NaN
df_cleaned = df_selected.dropna()

# Tampilkan informasi umum untuk memeriksa hasil pembersihan
df_cleaned.info()
# Tampilkan 5 baris pertama dari DataFrame yang telah dibersihkan
df_cleaned.head()

# Buat heatmap dari matriks korelasi
# Hitung matriks korelasi
corr_matrix = df_cleaned.corr()

# Buat heatmap menggunakan seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', cbar=True, square=True, linewidths=0.5, linecolor='black')
plt.title('Heatmap Korelasi')
plt.show()

"""6. kontruksi data"""

# Pastikan DataFrame yang digunakan adalah DataFrame yang sudah dibersihkan
df = df_cleaned.copy()

# Plot distribusi kelas sebelum SMOTE
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
df['target'].value_counts().plot(kind='bar', color=['blue', 'red', 'yellow', 'green'])
plt.title('Distribusi Kelas Sebelum Oversampling dengan SMOTE')
plt.xlabel('Kelas')
plt.ylabel('Jumlah')

# Memisahkan fitur dan target
X = df.drop('target', axis=1)
y = df['target']

# Menerapkan SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Plot distribusi kelas setelah SMOTE
plt.subplot(1, 2, 2)
pd.Series(y_res).value_counts().plot(kind='bar', color=['blue', 'red', 'yellow', 'green'])
plt.title('Distribusi Kelas Setelah Oversampling dengan SMOTE')
plt.xlabel('Kelas')
plt.ylabel('Jumlah')

plt.tight_layout()
plt.show()

"""7. pemodelan"""

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

# Standarisasi fitur
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Inisialisasi model
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Support Vector Machine": SVC(kernel='linear')
}

# Melatih dan mengevaluasi model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Model: {name}")
    print(f"Akurasi: {accuracy:.2f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\n")

from sklearn.model_selection import GridSearchCV

# Optimasi Hyperparameter untuk Logistic Regression
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear', 'saga']
}

grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5)
grid_search_lr.fit(X_train, y_train)

best_log_reg = grid_search_lr.best_estimator_
y_pred_lr = best_log_reg.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression (optimized) Akurasi: {accuracy_lr:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_lr))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

# Optimasi Hyperparameter untuk SVM
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma': ['scale', 'auto']
}

grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)
grid_search_svm.fit(X_train, y_train)

best_svm = grid_search_svm.best_estimator_
y_pred_svm = best_svm.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"SVM (optimized) Akurasi: {accuracy_svm:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Definisi grid parameter yang akan diuji
param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10],  # Menambahkan nilai 0.001 untuk mencoba regularization yang lebih kuat
    'solver': ['lbfgs', 'liblinear', 'saga'],
    'penalty': ['l1', 'l2']  # Menambahkan penalty options
}

# Membuat objek GridSearchCV
grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5)

# Melakukan training pada data training
grid_search_lr.fit(X_train, y_train)

# Mendapatkan model terbaik
best_log_reg = grid_search_lr.best_estimator_

# Melakukan prediksi pada data test
y_pred_lr = best_log_reg.predict(X_test)

# Menghitung akurasi
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print(f"Logistic Regression (optimized) Akurasi: {accuracy_lr:.2f}")

# Menampilkan classification report
print("Classification Report:")
print(classification_report(y_test, y_pred_lr))

# Menampilkan confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

"""8 evaluasi

1. Random Forest:

Awal: Model Random Forest awalnya memberikan akurasi sekitar 90%.

2. Logistic Regression:

Awal: Model Logistic Regression awalnya memiliki akurasi sekitar 60%.
Setelah Optimalisasi: Meskipun sudah dioptimalkan, akurasi Logistic Regression tetap sekitar 60%. Ada sedikit peningkatan dalam beberapa metrik seperti recall dan f1-score untuk beberapa kelas.

3. Support Vector Machine (SVM):

Awal: Model SVM awalnya memberikan akurasi sekitar 67%.
Setelah Optimalisasi: Akurasi SVM meningkat signifikan menjadi sekitar 92%. Terjadi peningkatan yang konsisten dalam semua metrik seperti precision, recall, dan f1-score untuk semua kelas.

Dengan begitu, Random Forest menunjukkan performa awal yang sangat baik dengan akurasi sekitar 90%. Sementara itu, Logistic Regression dan SVM mengalami perbaikan yang berbeda setelah dilakukan optimalisasi.

10. kesimpulan

Berdasarkan hasil evaluasi model yang saya lakukan:

Random Forest menunjukkan performa awal yang sangat baik dengan akurasi sekitar 90%. Ini menegaskan kemampuannya dalam mengolah dataset tanpa perlu optimalisasi tambahan yang disebutkan sebelumnya.

Logistic Regression, meskipun awalnya memiliki akurasi sekitar 60%, tidak mengalami peningkatan yang signifikan setelah proses optimalisasi. Meskipun begitu, model ini masih bermanfaat tergantung pada tujuan khusus dan keinginan untuk interpretasi yang lebih mudah dimengerti.

Support Vector Machine (SVM) mengalami peningkatan yang mencolok dari akurasi awal sekitar 67% menjadi sekitar 92% setelah optimalisasi. Hal ini menunjukkan bahwa SVM adalah pilihan yang sangat baik setelah melakukan penyetelan hyperparameter yang tepat.

Dengan demikian, pilihan model tergantung pada prioritas saya, apakah lebih mengutamakan performa awal yang kuat seperti Random Forest, peningkatan signifikan melalui optimalisasi seperti SVM, atau keinginan untuk model yang lebih mudah diinterpretasi seperti Logistic Regression.
"""